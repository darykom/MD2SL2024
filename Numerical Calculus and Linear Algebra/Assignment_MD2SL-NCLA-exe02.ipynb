{"cells":[{"cell_type":"markdown","metadata":{"id":"mAvS-CtMogHK"},"source":["# MD2SL - Master in Data Science and Statistical Learning\n","\n","**Numerical Calculus and Linear Algebra**\n","\n","Exercises 02: Linear systems: LU decomposition and partial pivoting\n","\n","Deadline: 01/05/2024"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXVusMheyD_D"},"outputs":[],"source":["# Installing packages\n","import numpy as np\n","import math\n","from scipy import *\n","import scipy.linalg as la\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"urnzqWnVxtpB"},"source":["# Exercise 1\n","Let $n\\in\\mathbb{N}$ and $\\widehat{\\mathbf{x}} = \\pmatrix{1.1 \\\\ \\vdots \\\\ 1.1}\\in\\mathbb{R}^n$.\n","\n","1. Consider the matrix $A = (a_{ij})\\in\\mathbb{R}^{n\\times n}$, so that\n","\\begin{equation}\n","  a_{ij} = \\cos(j\\theta),\\ \\text{with}\\ \\theta = \\frac{(2i+1)\\pi}{2n}\n","\\end{equation}\n","\n","  and the linear system $A\\cdot\\mathbf{x} = \\mathbf{b}$, with $\\mathbf{b} = A\\cdot\\widehat{\\mathbf{x}}$.\n","\n","  For $n=5,10,15,20,25,30$, solve the system using the functions `solveLU(A,b)` if possible, otherwise use `solveLUP(A,b)`.\n","  Print the following values, depending on $n$\n","  -  conditioning of the matrix\n","  -  euclidean norm of the relative error\n","  -  euclidean norm of the residual error\n","\n","  and comment the results.\n"]},{"cell_type":"markdown","metadata":{"id":"MHUFLCntA6IH"},"source":["Exercise 01 - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yd7W-VAkzFYa"},"outputs":[],"source":["# Matrix A\n","\n","def get_matrix_ex1(n):\n","    A = np.zeros((n, n))\n","    ctheta = math.pi/(2*n)\n","    for i in range(0,n):\n","        theta = (2*i+1)*ctheta\n","        for j in range(0,n):\n","            A[i,j] = math.cos(j*theta)\n","    return A"]},{"cell_type":"markdown","metadata":{"id":"4HoTSCnRKLL5"},"source":["Fattorizzazione LU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezZacfz-KKTc"},"outputs":[],"source":["# Fattorizzazione LU\n","def gaussLU(A):\n","    #Get the number of rows\n","    n = A.shape[0]\n","    LU = A.copy()\n","    #Loop over rows\n","    for k in range(n-1):\n","        if LU[k,k] == 0:\n","            print('A has no lu decomposition.')\n","            raise ValueError('A has no lu decomposition.')\n","        LU[k+1:,k]     = LU[k+1:,k]/LU[k,k] # calcolo moltiplicatori\n","        LU[k+1:,k+1:]  = LU[k+1:,k+1:]- np.outer(LU[k+1:,k],LU[k,k+1:]) # outer vector product\n","    return LU\n","\n","\n","def solveLU(LU, b):\n","  assert LU.shape[0] == LU.shape[1] # m == n, otherwise break\n","  n = LU.shape[0]\n","  assert b.shape[0] == n  # b length == n according to LU, otherwise break\n","  x = lowerTriangularUnitDiagonal(LU,b)\n","  x = upperTriangular(LU,x)\n","  rnorm = np.linalg.norm(b-(np.eye(n) + np.tril(LU,-1)) @ np.triu(LU) @ x, 2) # matrix multiplication with @\n","  return x, rnorm\n","\n","def lowerTriangularUnitDiagonal(A,b):\n","  assert A.shape[0] == A.shape[1] # m == n, otherwise break\n","  x = b.copy()\n","  for j in range(A.shape[1]):\n","    for i in range(j+1, A.shape[1]):\n","      x[i] = x[i]-A[i,j]*x[j]\n","  return x\n","\n","def upperTriangular(A,b):\n","  # A: upper triangular coefficient matrix\n","  # b: vector of costant terms\n","  x = b.copy()\n","  for j in range(A.shape[1]-1, -1, -1): # n-2:-1:0\n","    x[j] = x[j]/A[j,j]\n","    for i in range(j):\n","      x[i] = x[i] - A[i,j] * x[j]\n","  return x\n"]},{"cell_type":"markdown","metadata":{"id":"ZZ9qJseiKVPe"},"source":["Fattorizzazione LUP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdSxLGLrKZBP"},"outputs":[],"source":["def gaussLUP(A):\n","  LU = A.copy()\n","  n = LU.shape[0]\n","  piv = np.arange(0,n)\n","  for k in range(n-1):\n","    # pivoting\n","    r_idx = np.argmax(abs(LU[k:,k])) + k\n","    if LU[r_idx,k]==0:\n","        print('Singular Matrix')\n","        raise ValueError('Singular matrix.')\n","    piv[[k,r_idx]] = piv[[r_idx,k]]\n","    LU[[k,r_idx]] = LU[[r_idx,k]]\n","    # LU\n","    LU[k+1:,k]     = LU[k+1:,k]/LU[k,k]\n","    LU[k+1:,k+1:] = LU[k+1:,k+1:]- np.outer(LU[k+1:,k],LU[k,k+1:]) # outer vector product\n","  return LU, piv\n","\n","def solveLUP(LU,b,P):\n","  bp = b[P] # otherwise also b is changed\n","  x, rnorm = solveLU(LU,bp)\n","  return x, rnorm\n"]},{"cell_type":"markdown","metadata":{"id":"I_V4lj5JbM99"},"source":["Funzioni dei residui"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5K4Hwvi7bR8U"},"outputs":[],"source":["#print(f'Absolute error    : norm(x-A\\\\b)         =  {np.linalg.norm(x-x_hat):.2e}')\n","#print(f'Relative error    : norm(x-A\\\\b)/norm(x) =  {np.linalg.norm(x-x_hat)/np.linalg.norm(x):.2e}')\n","#print(f'Residual          : norm(b-A*x)         =   {rnorm}')\n","#print(f'Relative residual : norm(b-A*x)/norm(b) =   {rnorm/np.linalg.norm(b)}')\n","\n","def AbsoluteError(x, xex):\n","  return np.linalg.norm(x-xex)\n","\n","def RelativeError(x, xex):\n","  return np.linalg.norm(x-xex)/np.linalg.norm(xex)\n","\n","# superflua...\n","def Residual(A, x, b):\n","  return np.linalg.norm(b - A@b)\n","\n","def RelativeResidual(rnorm, b):\n","  return rnorm/np.linalg.norm(b)"]},{"cell_type":"markdown","metadata":{"id":"yvHRx4JjLkvL"},"source":["Main"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASUpYonN1AQK","outputId":"f2b4a63a-098f-4afb-a316-c46e783aeff5","executionInfo":{"status":"ok","timestamp":1716932912613,"user_tz":-120,"elapsed":375,"user":{"displayName":"Dario Comanducci","userId":"13956551013362018059"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------------------------------------------\n","n\t  cd\t\t  res\t\t   res-pivot\t rel-res\t  rel-res-piv\t  err\t\t   err-pivot\n","--------------------------------------------------------------------------------------------------------------------\n","5\t  1.41e+00\t  1.84e-15\t   6.59e-16\t 4.31e-16\t  1.55e-16\t  3.72e-16\t   2.39e-16\n","10\t  1.41e+00\t  1.18e-12\t   2.36e-15\t 1.45e-13\t  2.89e-16\t  2.04e-13\t   3.83e-16\n","15\t  1.41e+00\t  7.91e-10\t   4.45e-15\t 6.56e-11\t  3.69e-16\t  5.21e-11\t   4.33e-16\n","20\t  1.41e+00\t  1.60e-07\t   2.71e-14\t 1.01e-08\t  1.7e-15\t  7.11e-09\t   1.16e-15\n","25\t  1.41e+00\t  1.81e-04\t   1.21e-14\t 9.12e-06\t  6.1e-16\t  9.43e-06\t   7.22e-16\n","30\t  1.41e+00\t  2.78e-02\t   3.28e-14\t 0.00117\t  1.38e-15\t  1.04e-03\t   1.02e-15\n"]}],"source":["print('--------------------------------------------------------------------------------------------------------------------')\n","print('n\\t  cd\\t\\t  res\\t\\t   res-pivot\\t rel-res\\t  rel-res-piv\\t  err\\t\\t   err-pivot')\n","print('--------------------------------------------------------------------------------------------------------------------')\n","for n in range(5, 35, 5): # range(start, stop, step) -> [5, 10, 15, 20, 25, 30]\n","    A = get_matrix_ex1(n)\n","    x_ex = 1.1*np.ones((n,1)) # exact solution of the system\n","    b     = A @ x_ex # known vector\n","\n","    LU = gaussLU(A)\n","    x, rnorm = solveLU(LU, b)\n","    err_r = RelativeError(x, x_ex)\n","    rel_res = RelativeResidual(rnorm, b)\n","\n","    LU_piv, P = gaussLUP(A)\n","    x_piv, rnorm_piv = solveLUP(LU_piv, b, P)\n","    err_r_piv = RelativeError(x_piv, x_ex)\n","    rel_res_piv = RelativeResidual(rnorm_piv, b)\n","\n","\n","    print(f'{n}\\t  {np.linalg.cond(A):.2e}\\t  {rnorm:.2e}\\t   {rnorm_piv:.2e}\\t {rel_res:.3}\\t  {rel_res_piv:.3}\\t  {err_r:.2e}\\t   {err_r_piv:.2e}')"]},{"cell_type":"markdown","metadata":{"id":"DQI7160lx1hr"},"source":["- What happens to the condition number of $A$? What does it mean?\n","\n","La matrice $\\mathtt{A}$ rimane ben condizionata al crescere delle sue dimensioni, in quanto in questo caso il suo numero di condizionamento $k(\\mathtt{A})$ non varia rispetto al valore iniziale prossimo a 1.\n","\n","Posta $\\hat{\\mathbf{x}}$ la soluzione di ground-truth, ne segue che l'errore residuo relativo $\\frac{\\|\\mathtt{A}\\mathbf{x} - \\mathbf{b}\\|}{\\|\\mathbf{b}\\|}$ è una buona stima dell'errore relativo $\\frac{\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|}{\\|\\hat{\\mathbf{x}}\\|}$ in quanto\n","\\begin{equation}\n","\\frac{1}{k(\\mathtt{A})} \\frac{\\|\\mathbf{r}\\|}{\\|\\mathbf{b}\\|} \\le \\frac{\\|\\Delta \\mathbf{x}\\|}{\\|\\mathbf{x}\\|} \\le k(\\mathtt{A}) \\frac{\\|\\mathbf{r}\\|}{\\|\\mathbf{b}\\|}\n","\\end{equation}\n","con $\\mathbf{r} = \\mathtt{A}\\mathbf{x} - \\mathbf{b}$ e $\\Delta \\mathbf{x} = \\hat{\\mathbf{x}} - \\mathbf{x}$\n","\n","  \n","- How do the naive and pivot residuals behave?\n","\n","Il metodo con pivoting è migliore perché è possibile scegliere il divisore in modo da evitare per quanto possibile divisioni per valori piccoli nel processo di fattorizzazione.\n","\n","  \n","- How do the naive and pivot relative errors behave?\n","\n","Stesso pattern osservato sui residui, in virtù del basso numero di condizionamento: la soluzione ottenuta con pivoting è più accurata della fattorizzazione LU semplice\n"]},{"cell_type":"markdown","metadata":{"id":"HtLOzNMSxy5U"},"source":["# Exercise 2\n","Let $n\\in\\mathbb{N}$ and $\\widehat{\\mathbf{x}} = \\pmatrix{1.1 \\\\ \\vdots \\\\ 1.1}\\in\\mathbb{R}^n$.\n","\n","1. Consider the matrix $A = (a_{ij})\\in\\mathbb{R}^{n\\times n}$, so that\n","\\begin{equation}\n","  a_{ij} = (i+1)^{j}\n","\\end{equation}\n","\n","  and the linear system $A\\cdot\\mathbf{x} = \\mathbf{b}$, with $\\mathbf{b} = A\\cdot\\widehat{\\mathbf{x}}$.\n","\n","  For $n=1,\\dots,10$, solve the system using the functions `solveLU(A,b)` if possible, otherwise use `solveLUP(A,b)`.\n","  Print following values, depending on $n$\n","  -  conditioning of the matrix\n","  -  euclidean norm of the relative error\n","  -  euclidean norm of the residual error\n","\n","  and comment the results.\n"]},{"cell_type":"markdown","metadata":{"id":"RQTqlv08A_mO"},"source":["Exercise 02 - Solution\n","\n","Segue il codice che istanzia la matrice in funzione della dimensione n, e la stampa a video degli errori: oltre alla norma euclidea dell'errore relativo ($\\|\\mathbf{x}-\\hat{\\mathbf{x}}\\|/\\|\\hat{\\mathbf{x}}\\|$, con $\\hat{\\mathbf{x}}$ soluzione di ground thuth), e dell'errore residuo $\\|\\mathbf{b}-\\mathtt{A}\\mathbf{x}\\|$, si riporta anche il valore del residuo relativo $\\|\\mathbf{b}-\\mathtt{A}\\mathbf{x}\\|/\\|\\mathbf{b}\\|$ come ausilio alla comprensione dei dati tabulati."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZSvG3oWBCEb"},"outputs":[],"source":["def get_matrix_ex2(n):\n","    # TO DO: a_{ij} = (i+1)^j\n","    A = np.zeros((n, n))\n","    for i in range(0,n):\n","        A[i,0] = 1\n","        p = i+1\n","        for j in range(1,n):\n","            A[i,j] = p*A[i,j-1]\n","    return A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InzBn4ieCQig","executionInfo":{"status":"ok","timestamp":1716955955057,"user_tz":-120,"elapsed":272,"user":{"displayName":"Dario Comanducci","userId":"13956551013362018059"}},"outputId":"8daf227d-303e-4cf3-efc8-e42c55d01cac"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------------------------------------------\n","n\t  cd\t\t  res\t\t   res-pivot\t rel-res\t  rel-res-piv\t  err\t\t   err-pivot\n","--------------------------------------------------------------------------------------------------------------------\n","1\t  1.00e+00\t  0.00e+00\t   0.00e+00\t 0.0e+00\t  0.0e+00\t  0.00e+00\t   0.00e+00\n","2\t  6.85e+00\t  0.00e+00\t   0.00e+00\t 0.0e+00\t  0.0e+00\t  0.00e+00\t   0.00e+00\n","3\t  7.09e+01\t  0.00e+00\t   0.00e+00\t 0.0e+00\t  0.0e+00\t  8.72e-16\t   8.72e-16\n","4\t  1.17e+03\t  0.00e+00\t   1.59e-14\t 0.0e+00\t  1.5e-16\t  6.98e-15\t   9.55e-15\n","5\t  2.62e+04\t  0.00e+00\t   2.93e-14\t 0.0e+00\t  3.1e-17\t  5.21e-14\t   2.44e-13\n","6\t  7.31e+05\t  1.83e-12\t   9.45e-13\t 1.6e-16\t  8.4e-17\t  3.65e-12\t   1.48e-11\n","7\t  2.45e+07\t  2.29e-13\t   3.37e-11\t 1.4e-18\t  2.0e-16\t  2.23e-11\t   2.14e-10\n","8\t  9.52e+08\t  4.69e-10\t   5.01e-10\t 1.6e-16\t  1.7e-16\t  1.95e-09\t   3.95e-09\n","9\t  4.23e+10\t  3.75e-09\t   7.18e-09\t 6.5e-17\t  1.2e-16\t  1.24e-07\t   7.81e-07\n","10\t  2.11e+12\t  7.47e-09\t   3.16e-07\t 5.6e-18\t  2.4e-16\t  2.44e-06\t   3.71e-05\n"]}],"source":["print('--------------------------------------------------------------------------------------------------------------------')\n","print('n\\t  cd\\t\\t  res\\t\\t   res-pivot\\t rel-res\\t  rel-res-piv\\t  err\\t\\t   err-pivot')\n","print('--------------------------------------------------------------------------------------------------------------------')\n","\n","for n in range(1, 11): # range(start, stop, step) -> [5, 10, 15, 20, 25, 30]\n","\n","    A = get_matrix_ex2(n)\n","    x_hat = 1.1*np.ones((n,1)) # exact solution of the system\n","    b     = A @ x_hat # known vector\n","\n","    LU = gaussLU(A)\n","    x, rnorm = solveLU(LU, b)\n","    err_r = RelativeError(x, x_hat)\n","    rel_res = RelativeResidual(rnorm, b)\n","\n","    LU_piv, P = gaussLUP(A)\n","    x_piv, rnorm_piv = solveLUP(LU_piv, b, P)\n","    err_r_piv = RelativeError(x_piv, x_hat)\n","    rel_res_piv = RelativeResidual(rnorm_piv, b)\n","\n","    #print(f'{n}  {np.linalg.cond(A):.2e}  {rnorm:.2e}   {rnorm_piv:.2e}   {err_r:.2e}   {err_r_piv:.2e}')\n","    print(f'{n}\\t  {np.linalg.cond(A):.2e}\\t  {rnorm:.2e}\\t   {rnorm_piv:.2e}\\t {rel_res:.1e}\\t  {rel_res_piv:.1e}\\t  {err_r:.2e}\\t   {err_r_piv:.2e}')"]},{"cell_type":"markdown","metadata":{"id":"K9jpNfoGEdK2"},"source":["- What happens to the condition number of $A$? What does it mean?\n","\n","La matrice diventa sempre più malcondizionata al crescere di n, per cui l'errore residuo relativo $\\frac{\\|\\mathtt{A}\\mathbf{x} - \\mathbf{b}\\|}{\\|\\mathbf{b}\\|}$ non è un buon indicatore della bontà della stima $\\mathbf{x}$ rispetto al ground truth $\\hat{\\mathbf{x}}$ (ossia $\\frac{\\|\\hat{\\mathbf{x}} - \\mathbf{x}\\|}{\\|\\hat{\\mathbf{x}}\\|}$)\n","\n","- How do the naive and pivot residuals behave?\n","\n","La LU che mostra residui il più delle volte più bassi della LUP.\n","\n","- How do the naive and pivot relative errors behave?\n","\n","Il metodo senza pivoting produce errori relativi inferiori, talvolta anche di 1 ordine di grandezza: volendo azzardare una spiegazione del fenomeno, l'ipotesi per questo fatto è che la struttura della matrice A, a valori crescenti esponenzialmente spostandoci verso l'angolo inferiore a destra, influisca negativamente negli errori di approssimazione fatti permutando le righe alla ricerca del pivoting più grande da usare nel processo di riduzione della matrice."]},{"cell_type":"markdown","metadata":{"id":"-2-RN9cRx3Qf"},"source":["# Exercise 3\n","Let $A\\in\\mathbb{R}^{n\\times n}$ a tridiagonal matrix\n","\\begin{equation}\n","  A = \\pmatrix{d_0     & a_0    &        &         & \\huge 0 \\\\\n","               c_1     & \\ddots & \\ddots &         &         \\\\\n","                       & \\ddots & \\ddots & \\ddots  &         \\\\\n","                       &        & \\ddots & \\ddots  & a_{n-2} \\\\\n","               \\huge 0 &        &        & c_{n-1} & d_{n-1} \\\\}\n","\\end{equation}\n","\n","for which its LU decomposition exists. Hence, $A = L\\cdot U$ with\n","\\begin{equation}\n","L = \\pmatrix{1       &        &         & \\huge0 \\\\\n","             l_1     & \\ddots &         &        \\\\\n","                     & \\ddots & \\ddots  &        \\\\\n","             \\huge 0 &        & l_{n-1} & 1},\n","\\quad\n","U = \\pmatrix{u_0       &  a_0   &         & \\huge0 \\\\\n","                     & \\ddots & \\ddots  &        \\\\\n","                     &        & \\ddots  & a_{n-2}\\\\\n","             \\huge 0 &        &         & u_{n-1}}.\n","\\end{equation}\n","\n","\n","Example given for $n=4$.\n","\\begin{equation}\n","\\pmatrix{d_0 & a_0 & 0   & 0   \\\\\n","         c_1 & d_1 & a_1 & 0   \\\\\n","         0   & c_2 & d_2 & a_2 \\\\\n","         0   & 0   & c_3 & d_3} = \\pmatrix{1   & 0   & 0   & 0 \\\\\n","                                           l_2 & 1   & 0   & 0 \\\\\n","                                           0   & l_3 & 1   & 0 \\\\\n","                                           0   & 0   & l_4 & 1 }\\pmatrix{u_0   & a_0 & 0   & 0   \\\\\n"," 0     & u_1 & a_1 & 0   \\\\\n"," 0     & 0   & u_2 & a_2 \\\\\n"," 0     & 0   & 0   & u_3  }\n","\\end{equation}\n","\n","\n","In particular,\n","\\begin{array}{clll}\n","          & c_1 = l_1u_0        &  c_2 = l_2u_1       & c_3 = l_3u_2\\\\\n","d_0 = u_0 & d_1 = l_1a_0 + u_1  &  d_2 = l_2a_1 + u_2 & d_3 = l_3a_2 + u_3.\\\\\n","\\end{array}\n","\n","\n","\n","1. Write a python function `thomas(c,d,a)` which takes in input the diagonals   of a tridiagonal square matrix $A\\in\\mathbb{R}^{n\\times n}$\n","  - $c:$  lower diagonal\n","  - $d:$  main diagonal\n","  - $a:$  upper diagonal\n","  \n","  and returns the diagonals of the lu decomposition $A = L\\cdot U$\n","  - $l:$  lower diagonal of $L$\n","  - $u:$  upper diagonal of $U$\n","\n","  by implementing the following *Thomas algorithm*.\n","\n","  **Input:** $c$, $d$, $a$\n","  1. $u_0$ = $d_0$\n","  2. for $i=1,\\dots,n-1$\n","    - $l_i$ = $c_i / u_{i-1}$\n","    - $u_i$ = $d_i - l_ia_{i-1}$\n","  **Output:** $l$, $u$\n","\n","2. Test the function `thomas(c, d, a)` on the following tridiagonal matrices.\n","\n","\\begin{equation}\n","\\begin{split}\n","(a)\\ \\pmatrix{1 & 4 & 0 & 0\\\\\n","             3 & 4 & 1 & 0\\\\\n","             0 & 2 & 3 & 4\\\\\n","             0 & 0 & 1 & 3}\n","             \\\\\n","             \\\\\n","(b)\\ \\pmatrix{2  & 1  & 0  & 0\\\\\n","             -1 & 2  & 1  & 0\\\\\n","             0  & -1 & 2  & 1\\\\\n","             0  & 0  & -1 & 2\n","            }\n","            \\\\\n","            \\\\\n","(c)\\ \\pmatrix{ 2 &  1 &  0 &  0 &  0 & 0\\\\\n","              -1 &  4 &  1 &  0 &  0 & 0\\\\\n","               0 & -1 &  4 &  1 &  0 & 0\\\\\n","               0 &  0 & -1 &  4 &  1 & 0\\\\\n","               0 &  0 &  0 & -1 &  4 & 1\\\\\n","               0 &  0 &  0 &  0 & -1 & 2}\n","\\end{split}\n","\\end{equation}\n","  - reconstruct the $L$ and $U$ matricies of their LU decomposition;\n","  - compute the recontruction error as $\\texttt{norm}(A - L\\cdot U)$."]},{"cell_type":"markdown","metadata":{"id":"nOWbstqHYgfi"},"source":["Exercise 3.1 - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UZu219JoRhO"},"outputs":[],"source":["def thomas(c, d, a):\n","    # TO DO\n","    n = len(d)\n","    u = np.ones(n)\n","    l = np.ones(n-1)\n","    u[0] = d[0]\n","    for i in range(1,n):\n","        iprv = i-1\n","        l[iprv] = c[iprv]/u[iprv]\n","        u[i] = d[i]-l[iprv]*a[iprv]\n","    return l, u\n"]},{"cell_type":"markdown","metadata":{"id":"scn1yL057FJJ"},"source":["Exercise 3.1.a - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwvKQrHraGg5","executionInfo":{"status":"ok","timestamp":1716957324599,"user_tz":-120,"elapsed":266,"user":{"displayName":"Dario Comanducci","userId":"13956551013362018059"}},"outputId":"21dec899-78ae-46ef-fb6e-a6b8e994c14b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstruction error: 0.0\n"]}],"source":["A = np.array([[2,1,0,0], [3,4,1,0], [0,2,3,4], [0,0,1,3]])\n","\n","d = np.diag(A) # main diagonal of A\n","c = np.diag(A, k=-1) # lower diagonal of A\n","a = np.diag(A, k=1) # upper diagonal of A\n","\n","#l, u = thomas(lower,diag,upper)\n","l, u = thomas(c,d,a)\n","\n","n = A.shape[0]\n","L = np.diag(np.ones(n),0) + np.diag(l,-1) # assemble\n","U = np.diag(u,0) + np.diag(a,1) # assemble\n","\n","error = np.linalg.norm(A-L@U)\n","\n","print(f'Reconstruction error: {error}')"]},{"cell_type":"markdown","metadata":{"id":"ul4VlBMu7JJj"},"source":["Exercise 3.1.b - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"En_JHOHcgXBv","executionInfo":{"status":"ok","timestamp":1716957326923,"user_tz":-120,"elapsed":408,"user":{"displayName":"Dario Comanducci","userId":"13956551013362018059"}},"outputId":"7e63244d-d94a-4664-d1c0-5ed71b03446f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstruction error: 2.220446049250313e-16\n"]}],"source":["A = np.array([[1,4,0,0], [-1,2,1,0], [0,-1,2,1], [0,0,-1,2]])\n","\n","\n","d = np.diag(A) # main diagonal of A\n","c = np.diag(A, k=-1) # lower diagonal of A\n","a = np.diag(A, k=1) # upper diagonal of A\n","\n","#l, u = thomas(lower,diag,upper)\n","l, u = thomas(c,d,a)\n","\n","n = A.shape[0]\n","L = np.diag(np.ones(n),0) + np.diag(l,-1) # assemble\n","U = np.diag(u,0) + np.diag(a,1) # assemble\n","\n","error = np.linalg.norm(A-L@U)\n","\n","print(f'Reconstruction error: {error}')"]},{"cell_type":"markdown","metadata":{"id":"WfLCM4qQ7LPx"},"source":["Exercise 3.1.c - Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9ljiZGwhtmP","executionInfo":{"status":"ok","timestamp":1716957597834,"user_tz":-120,"elapsed":286,"user":{"displayName":"Dario Comanducci","userId":"13956551013362018059"}},"outputId":"36f73d42-241f-40b5-e808-fdd9998087be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reconstruction error: 6.280369834735101e-16\n"]}],"source":["dA = [2, 4, 4, 4, 4, 2]\n","uA = np.ones(5)\n","lA = -np.ones(5)\n","A = np.diag(lA,-1) + np.diag(dA,0) + np.diag(uA,1)\n","\n","d = np.diag(A) # main diagonal of A\n","c = np.diag(A, k=-1) # lower diagonal of A\n","a = np.diag(A, k=1) # upper diagonal of A\n","\n","#l, u = thomas(lower,diag,upper)\n","l, u = thomas(c,d,a)\n","\n","n = A.shape[0]\n","L = np.diag(np.ones(n),0) + np.diag(l,-1) # assemble\n","U = np.diag(u,0) + np.diag(a,1) # assemble\n","\n","error = np.linalg.norm(A-L@U)\n","\n","print(f'Reconstruction error: {error}')"]}],"metadata":{"colab":{"provenance":[{"file_id":"1xL_2RkXlzj25EtbESjgw-z_GYbxm87TA","timestamp":1716208677512}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}